{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPU_Training-Alias-Free_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdV29qH7cD47w+xSJUF3vZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duskvirkus/alias-free-gan/blob/notebook-update/notebooks/GPU_Training_Alias_Free_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iooMpU0wSq1v"
      },
      "source": [
        "# GPU Training - Alias-Free GAN\n",
        "by duskvirkus\n",
        "\n",
        "This is a notebook for training Alias-Free GAN on a Colab GPU instance.\n",
        "\n",
        "Repository: https://github.com/duskvirkus/alias-free-gan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ygaIX_TP7A"
      },
      "source": [
        "# GPU check\n",
        "\n",
        "If this fails change the runtime type in `Runtime > Change runtime type > Select GPU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFKRHS3TTPbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1d8295-3ce6-42e3-9b61-d67690179f6d"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-f1e107b6-811c-edc1-fc11-08cae0acfb96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRc3UyhPTi_6"
      },
      "source": [
        "## Connect Google Drive\n",
        "\n",
        "This notebook is designed to be used with google drive connected. If you'd like to use it without google drive you'll have to make changes.\n",
        "\n",
        "The main reason behind this is Colab sessions automaticall shut off after a number of hours (~10 for free and ~20 for pro). This risks loosing training progress if it's not saved to persistent storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t1M2VB4Tif6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea91c9ed-95f0-45ad-b69e-7c5c2d8b7ac8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPv1ThsOU-Op"
      },
      "source": [
        "## Clone / cd into Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NBaGNEbSqPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b11b4a2-1cd1-4e15-e92b-e5c78b63789d"
      },
      "source": [
        "import os\n",
        "drive_path = '/content/drive/MyDrive/'\n",
        "repo_container_dir = 'colab-alias-free-gan'\n",
        "repo_name = 'alias-free-gan'\n",
        "git_repo = 'https://github.com/duskvirkus/alias-free-gan.git'\n",
        "\n",
        "working_dir = os.path.join(drive_path, repo_container_dir, repo_name)\n",
        "\n",
        "if os.path.isdir(working_dir):\n",
        "  %cd {working_dir}\n",
        "else:\n",
        "  container_path = os.path.join(drive_path, repo_container_dir)\n",
        "  os.makedirs(container_path)\n",
        "  %cd {container_path}\n",
        "  !git clone {git_repo}\n",
        "  %cd {repo_name}\n",
        "  !mkdir pretrained"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab-alias-free-gan\n",
            "Cloning into 'alias-free-gan'...\n",
            "remote: Enumerating objects: 1146, done.\u001b[K\n",
            "remote: Counting objects: 100% (188/188), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 1146 (delta 98), reused 136 (delta 74), pack-reused 958\u001b[K\n",
            "Receiving objects: 100% (1146/1146), 73.49 MiB | 20.55 MiB/s, done.\n",
            "Resolving deltas: 100% (559/559), done.\n",
            "Checking out files: 100% (94/94), done.\n",
            "/content/drive/MyDrive/colab-alias-free-gan/alias-free-gan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6qSNzMhXQta"
      },
      "source": [
        "## Install Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZYccZIHSpNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa49861b-f6db-4b30-cc96-cde54d9834df"
      },
      "source": [
        "!python install.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.4.2-py3-none-any.whl (916 kB)\n",
            "\u001b[K     |████████████████████████████████| 916 kB 15.9 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning-bolts\n",
            "  Downloading pytorch_lightning_bolts-0.3.2-py3-none-any.whl (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 57.8 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.10.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 64.3 MB/s \n",
            "\u001b[?25hCollecting pyhocon\n",
            "  Downloading pyhocon-0.3.58.tar.gz (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 73.9 MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting opensimplex\n",
            "  Downloading opensimplex-0.3-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Collecting torchmetrics>=0.4.0\n",
            "  Downloading torchmetrics-0.5.0-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 68.9 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.5.0)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 64.3 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 71.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 70.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.5)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.34.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.34.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.6.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 62.1 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.8.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 59.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.5.0)\n",
            "Building wheels for collected packages: future, subprocess32, pyhocon, pathtools\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=83d6a02126b69116876fb416bef3d64d201ba8abc87cfa2942f9561f4a992eb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=4079bf3921c0306e1f012505a96d9d9e868c3623957b6e0cd54f5d80f4331c41\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhocon: filename=pyhocon-0.3.58-py3-none-any.whl size=19890 sha256=2d2c6c5975e05fbc61d1fb2d635269386a7c2f2894fa696587198269f31169c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/20/f9/ff360765ce6f9fc078d6599c10a8f36496e5b5011a29df1ae3\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=e498fcc946e6a758318959d67cf5265125e4f880ae7cae8c8c005ca810e58781\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built future subprocess32 pyhocon pathtools\n",
            "Installing collected packages: multidict, yarl, async-timeout, smmap, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, gitdb, future, subprocess32, shortuuid, sentry-sdk, pytorch-lightning, pathtools, GitPython, docker-pycreds, configparser, wandb, pytorch-lightning-bolts, pyhocon, pydantic, opensimplex, opencv-python-headless, ninja\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed GitPython-3.1.18 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 configparser-5.0.2 docker-pycreds-0.4.0 fsspec-2021.7.0 future-0.18.2 gitdb-4.0.7 multidict-5.1.0 ninja-1.10.2 opencv-python-headless-4.5.3.56 opensimplex-0.3 pathtools-0.1.2 pyDeprecate-0.3.1 pydantic-1.8.2 pyhocon-0.3.58 pytorch-lightning-1.4.2 pytorch-lightning-bolts-0.3.2 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 torchmetrics-0.5.0 wandb-0.12.0 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdzlUBR5XanA"
      },
      "source": [
        "## Convert Dataset\n",
        "\n",
        "You can skip this section if you already have a dataset in the correct format.\n",
        "\n",
        "Currently only supports datasets with only one of the following dimensions of images. 256 by 256 **or** 512 by 512 **or** 1024 by 1024\n",
        "\n",
        "Preparing your dataset for conversion. Tools to prep a data set are beyond the scope of this notebook dvschultz/dataset-tools(https://github.com/dvschultz/dataset-tools) is suggested to help with this process.\n",
        "\n",
        "Structure of your dataset:\n",
        "```\n",
        "dataset_root_dir # name of your dataset is suggested\n",
        "  |- sub_directory # anything (this has to do with labels which is an unsupported feature at current time)\n",
        "    |- image01.png\n",
        "    |- images_can_have_any_names.png\n",
        "    |- they_also_be.jpg\n",
        "    |...continued # Suggested minimum size is 1000+ images.\n",
        "```\n",
        "\n",
        "The above example would result in an input of `unconverted_dataset='path/to/dataset_root_dir'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqswLFEkarPc",
        "outputId": "7e332047-f216-42a1-b07b-cf3a28335637"
      },
      "source": [
        "unconverted_dataset = '/content/drive/MyDrive/dataset-creation/painterly-faces-v2'\n",
        "out_path = '/content/drive/MyDrive/datasets-aliasfree/painterly-faces-v2-256'\n",
        "dataset_size = 256 # one of the following 256, 512, 1024\n",
        "!python scripts/convert_dataset.py --size {dataset_size} {unconverted_dataset} {out_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Make dataset of image sizes: 256\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:387: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "1it [00:00,  1.60it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:387: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "1158it [06:45,  2.85it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6Yc1QbacaId"
      },
      "source": [
        "## Info on training options\n",
        "\n",
        "Most training options work rather well out of the box. See the training section for suggested arguments.\n",
        "\n",
        "You can see a full list of training options by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuc-24U3dH_l"
      },
      "source": [
        "!python scripts/trainer.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMt7IazBas5c"
      },
      "source": [
        "## Training\n",
        "\n",
        "Results from training can be found in `results` directory.\n",
        "\n",
        "**Resume from Checkpoint**\n",
        "\n",
        "Set `--resume_from 'path/to/checkpoint.pt'`\n",
        "\n",
        "If resuming from a checkpoint that doesn't use the new kimg naming scheme use `--start_kimg_count` to set the starting count manually.\n",
        "\n",
        "**Transfer Learning Options**\n",
        "\n",
        "See repository for transfer learning options. https://github.com/duskvirkus/alias-free-gan/blob/devel/pretrained_models.json\n",
        "\n",
        "Use `--resume_from 'model_name'`. wget is used to automatically download the pretrained models.\n",
        "\n",
        "**Training from Scratch**\n",
        "\n",
        "This is not recommended as transfer learning off of any model even if it's not related to your dataset will be faster and consume less resources. Unless there is no pretrained models or you have an explicit reason use transfer learning. To train from scratch simply leave resume blank, like so `--resume_from ''`.\n",
        "\n",
        "### Suggested Batch Size\n",
        "\n",
        "For colab pro gpus (16GB) here are the suggested batch sizes:\n",
        "- 256: batch size 8 recommended\n",
        "- 512: batch size 4 recommended\n",
        "- 1024: batch size 4 recommended\n",
        "\n",
        "Feel free to play around to see if you can get things higher. For the best performance try to keep batch in powers of 2.\n",
        "\n",
        "### Trouble Shooting\n",
        "\n",
        "If you get a cuda out of memory error try reducing the `batch`.\n",
        "\n",
        "If you get another error please report it at https://github.com/duskvirkus/alias-free-gan/issues/new\n",
        "\n",
        "If the model makes it through the first epoch you're unlike to encounter any errors after that.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI69L2vybsPr"
      },
      "source": [
        "model_size = 256\n",
        "dataset_location = '/content/drive/MyDrive/datasets-aliasfree/painterly-faces-v2-256'\n",
        "resume = 'rosinality-ffhq-800k'\n",
        "batch_size = 8\n",
        "\n",
        "sample_frequency = 1 # in kimgs or thousands of images\n",
        "checkpoint_frequency = 4 # in kimgs or thousands of images"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avx9vyhlczji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992d31c5-490c-4f0b-d4c7-540eaeef6fbd"
      },
      "source": [
        "!python scripts/trainer.py \\\n",
        "    --gpus 1 \\\n",
        "    --size {model_size} \\\n",
        "    --dataset_path {dataset_location} \\\n",
        "    --resume_from {resume} \\\n",
        "    --batch {batch_size} \\\n",
        "    --save_sample_every_kimgs {sample_frequency} \\\n",
        "    --save_checkpoint_every_kimgs {checkpoint_frequency}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Alias-Free GAN version: 1.0.0\n",
            "\n",
            "\n",
            "Licence and compensation information for rosinality-ffhq-800k pretrained model: test information\n",
            "\n",
            "\n",
            "Dataset path: /content/drive/MyDrive/datasets-aliasfree/painterly-faces-v2-256\n",
            "Initialized MultiResolutionDataset dataset with 1158 images\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "2021-08-16 04:37:53.526893: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\n",
            "  | Name          | Type          | Params\n",
            "------------------------------------------------\n",
            "0 | generator     | Generator     | 17.3 M\n",
            "1 | g_ema         | Generator     | 17.3 M\n",
            "2 | discriminator | Discriminator | 28.9 M\n",
            "------------------------------------------------\n",
            "63.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "63.5 M    Total params\n",
            "253.864   Total estimated model params size (MB)\n",
            "Training: -1it [00:00, ?it/s]\n",
            "\n",
            "Resuming from: /content/drive/My Drive/colab-alias-free-gan/alias-free-gan/scripts/../pretrained/rosinality-ffhq-800k.pt\n",
            "\n",
            "AlignFreeGAN device: cuda:0\n",
            "\n",
            "\n",
            "Epoch 0:   0% 0/144 [00:00<00:00, 3968.12it/s]  /content/drive/My Drive/colab-alias-free-gan/alias-free-gan/scripts/../src/stylegan2/op/conv2d_gradfix.py:89: UserWarning: conv2d_gradfix not supported on PyTorch 1.9.0+cu102. Falling back to torch.nn.functional.conv2d().\n",
            "  f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\n",
            "Epoch 4:  56% 80/144 [01:20<01:03,  1.00it/s, kimgs=5.240]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1047: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
            "Epoch 4:  56% 80/144 [01:24<01:06,  1.04s/it, kimgs=5.240]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}