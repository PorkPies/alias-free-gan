{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!python --version"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python 3.7.11\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%cd /content/drive/MyDrive/alias-free-devel2/alias-free-gan"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/alias-free-devel2/alias-free-gan\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python install.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model_size = 256\n",
    "resume = ''\n",
    "dataset_location = '/content/drive/MyDrive/alias-free-devel2/alias-free-gan/alias-free-gan-ci-files/flowers-test-dataset-32-256'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "!python scripts/trainer.py --help"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using Alias-Free GAN version: 1.0.0\n",
      "usage: trainer.py [-h] --dataset_path DATASET_PATH [--resume_from RESUME_FROM]\n",
      "                  --size SIZE [--batch BATCH] [--n_samples N_SAMPLES]\n",
      "                  [--lr_g LR_G] [--lr_d LR_D] [--d_reg_every D_REG_EVERY]\n",
      "                  [--r1 R1] [--augment AUGMENT] [--argument_p ARGUMENT_P]\n",
      "                  [--ada_target ADA_TARGET] [--ada_length ADA_LENGTH]\n",
      "                  [--ada_every ADA_EVERY]\n",
      "                  [--stylegan2_discriminator STYLEGAN2_DISCRIMINATOR]\n",
      "                  [--save_sample_every_kimgs SAVE_SAMPLE_EVERY_KIMGS]\n",
      "                  [--save_checkpoint_every_kimgs SAVE_CHECKPOINT_EVERY_KIMGS]\n",
      "                  [--start_kimg_count START_KIMG_COUNT]\n",
      "                  [--stop_training_at_kimgs STOP_TRAINING_AT_KIMGS]\n",
      "                  [--sample_grid SAMPLE_GRID] [--logger [LOGGER]]\n",
      "                  [--checkpoint_callback [CHECKPOINT_CALLBACK]]\n",
      "                  [--default_root_dir DEFAULT_ROOT_DIR]\n",
      "                  [--gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                  [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                  [--process_position PROCESS_POSITION]\n",
      "                  [--num_nodes NUM_NODES] [--num_processes NUM_PROCESSES]\n",
      "                  [--devices DEVICES] [--gpus GPUS]\n",
      "                  [--auto_select_gpus [AUTO_SELECT_GPUS]]\n",
      "                  [--tpu_cores TPU_CORES] [--ipus IPUS]\n",
      "                  [--log_gpu_memory LOG_GPU_MEMORY]\n",
      "                  [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]\n",
      "                  [--overfit_batches OVERFIT_BATCHES]\n",
      "                  [--track_grad_norm TRACK_GRAD_NORM]\n",
      "                  [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                  [--fast_dev_run [FAST_DEV_RUN]]\n",
      "                  [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                  [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                  [--max_steps MAX_STEPS] [--min_steps MIN_STEPS]\n",
      "                  [--max_time MAX_TIME]\n",
      "                  [--limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                  [--limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                  [--limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                  [--limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                  [--val_check_interval VAL_CHECK_INTERVAL]\n",
      "                  [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]\n",
      "                  [--log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                  [--accelerator ACCELERATOR]\n",
      "                  [--sync_batchnorm [SYNC_BATCHNORM]] [--precision PRECISION]\n",
      "                  [--weights_summary WEIGHTS_SUMMARY]\n",
      "                  [--weights_save_path WEIGHTS_SAVE_PATH]\n",
      "                  [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                  [--truncated_bptt_steps TRUNCATED_BPTT_STEPS]\n",
      "                  [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                  [--profiler PROFILER] [--benchmark [BENCHMARK]]\n",
      "                  [--deterministic [DETERMINISTIC]]\n",
      "                  [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
      "                  [--reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]]\n",
      "                  [--auto_lr_find [AUTO_LR_FIND]]\n",
      "                  [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]\n",
      "                  [--terminate_on_nan [TERMINATE_ON_NAN]]\n",
      "                  [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
      "                  [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]\n",
      "                  [--plugins PLUGINS] [--amp_backend AMP_BACKEND]\n",
      "                  [--amp_level AMP_LEVEL]\n",
      "                  [--distributed_backend DISTRIBUTED_BACKEND]\n",
      "                  [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
      "                  [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]\n",
      "                  [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "Trainer Script:\n",
      "  --dataset_path DATASET_PATH\n",
      "                        Path to dataset. Required!\n",
      "  --resume_from RESUME_FROM\n",
      "                        Resume from checkpoint or transfer learn off\n",
      "                        pretrained model. Leave blank to train from scratch.\n",
      "\n",
      "AliasFreeGAN Model:\n",
      "  --size SIZE           Pixel dimension of model. Must be 256, 512, or 1024.\n",
      "                        Required!\n",
      "  --batch BATCH         Batch size. Will be overridden if\n",
      "                        --auto_scale_batch_size is used. (default: 16)\n",
      "  --n_samples N_SAMPLES\n",
      "                        Number of samples to generate in training process. Be\n",
      "                        sure to put --n_samples_off_batch False to use\n",
      "                        otherwise samples will be the same as batch. (default:\n",
      "                        8)\n",
      "  --lr_g LR_G           Generator learning rate. (default: 0.002)\n",
      "  --lr_d LR_D           Discriminator learning rate. (default: 0.002)\n",
      "  --d_reg_every D_REG_EVERY\n",
      "                        Regularize discriminator ever _ iters. (default: 16)\n",
      "  --r1 R1               R1 regularization weights. (default: 10.0)\n",
      "  --augment AUGMENT     Use augmentations. (default: False)\n",
      "  --argument_p ARGUMENT_P\n",
      "                        (default: 0.0)\n",
      "  --ada_target ADA_TARGET\n",
      "                        (default: 0.6)\n",
      "  --ada_length ADA_LENGTH\n",
      "                        (default: 500000)\n",
      "  --ada_every ADA_EVERY\n",
      "                        (default: 256)\n",
      "  --stylegan2_discriminator STYLEGAN2_DISCRIMINATOR\n",
      "                        Provide path to a rosinality stylegan2 checkpoint to\n",
      "                        load the discriminator from it. Will load second so if\n",
      "                        you load another model first it will override that\n",
      "                        discriminator.\n",
      "\n",
      "kimg Saver Callback:\n",
      "  --save_sample_every_kimgs SAVE_SAMPLE_EVERY_KIMGS\n",
      "                        Sets the frequency of saving samples in kimgs\n",
      "                        (thousands of image). (default: 1)\n",
      "  --save_checkpoint_every_kimgs SAVE_CHECKPOINT_EVERY_KIMGS\n",
      "                        Sets the frequency of saving model checkpoints in\n",
      "                        kimgs (thousands of image). (default: 4)\n",
      "  --start_kimg_count START_KIMG_COUNT\n",
      "                        Manually override the start count for kimgs. If not\n",
      "                        set the count will be inferred from checkpoint name.\n",
      "                        If count can not be inferred it will default to 0.\n",
      "  --stop_training_at_kimgs STOP_TRAINING_AT_KIMGS\n",
      "                        Automatically stop training at this number of kimgs.\n",
      "                        (default: 12800)\n",
      "  --sample_grid SAMPLE_GRID\n",
      "                        Sample grid to use for samples. Saved under\n",
      "                        assets/sample_grids. (default:\n",
      "                        default_5x3_sample_grid)\n",
      "\n",
      "pl.Trainer:\n",
      "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\n",
      "                        experiment tracking. A ``True`` value uses the default\n",
      "                        ``TensorBoardLogger``. ``False`` will disable logging.\n",
      "                        If multiple loggers are provided and the `save_dir`\n",
      "                        property of that logger is not set, local files\n",
      "                        (checkpoints, profiler traces, etc.) are saved in\n",
      "                        ``default_root_dir`` rather than in the ``log_dir`` of\n",
      "                        any of the individual loggers.\n",
      "  --checkpoint_callback [CHECKPOINT_CALLBACK]\n",
      "                        If ``True``, enable checkpointing. It will configure a\n",
      "                        default ModelCheckpoint callback if there is no user-\n",
      "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\n",
      "                        ng.trainer.trainer.Trainer.callbacks`.\n",
      "  --default_root_dir DEFAULT_ROOT_DIR\n",
      "                        Default path for logs and weights when no\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\n",
      "                        or 'hdfs://path/'\n",
      "  --gradient_clip_val GRADIENT_CLIP_VAL\n",
      "                        0 means don't clip.\n",
      "  --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n",
      "                        'value' means clip_by_value, 'norm' means\n",
      "                        clip_by_norm. Default: 'norm'\n",
      "  --process_position PROCESS_POSITION\n",
      "                        orders the progress bar when running multiple models\n",
      "                        on same machine.\n",
      "  --num_nodes NUM_NODES\n",
      "                        number of GPU nodes for distributed training.\n",
      "  --num_processes NUM_PROCESSES\n",
      "                        number of processes for distributed training with\n",
      "                        distributed_backend=\"ddp_cpu\"\n",
      "  --devices DEVICES     Will be mapped to either `gpus`, `tpu_cores`,\n",
      "                        `num_processes` or `ipus`, based on the accelerator\n",
      "                        type.\n",
      "  --gpus GPUS           number of gpus to train on (int) or which GPUs to\n",
      "                        train on (list or str) applied per node\n",
      "  --auto_select_gpus [AUTO_SELECT_GPUS]\n",
      "                        If enabled and `gpus` is an integer, pick available\n",
      "                        gpus automatically. This is especially useful when\n",
      "                        GPUs are configured to be in \"exclusive mode\", such\n",
      "                        that only one process at a time can access them.\n",
      "  --tpu_cores TPU_CORES\n",
      "                        How many TPU cores to train on (1 or 8) / Single TPU\n",
      "                        to train on [1]\n",
      "  --ipus IPUS           How many IPUs to train on.\n",
      "  --log_gpu_memory LOG_GPU_MEMORY\n",
      "                        None, 'min_max', 'all'. Might slow performance\n",
      "  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE\n",
      "                        How often to refresh progress bar (in steps). Value\n",
      "                        ``0`` disables progress bar. Ignored when a custom\n",
      "                        progress bar is passed to\n",
      "                        :paramref:`~Trainer.callbacks`. Default: None, means a\n",
      "                        suitable value will be chosen based on the environment\n",
      "                        (terminal, Google COLAB, etc.).\n",
      "  --overfit_batches OVERFIT_BATCHES\n",
      "                        Overfit a fraction of training data (float) or a set\n",
      "                        number of batches (int).\n",
      "  --track_grad_norm TRACK_GRAD_NORM\n",
      "                        -1 no tracking. Otherwise tracks that p-norm. May be\n",
      "                        set to 'inf' infinity-norm.\n",
      "  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n",
      "                        Check val every n train epochs.\n",
      "  --fast_dev_run [FAST_DEV_RUN]\n",
      "                        runs n if set to ``n`` (int) else 1 if set to ``True``\n",
      "                        batch(es) of train, val and test to find any bugs (ie:\n",
      "                        a sort of unit test).\n",
      "  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n",
      "                        Accumulates grads every k batches or as set up in the\n",
      "                        dict.\n",
      "  --max_epochs MAX_EPOCHS\n",
      "                        Stop training once this number of epochs is reached.\n",
      "                        Disabled by default (None). If both max_epochs and\n",
      "                        max_steps are not specified, defaults to\n",
      "                        ``max_epochs`` = 1000.\n",
      "  --min_epochs MIN_EPOCHS\n",
      "                        Force training for at least these many epochs.\n",
      "                        Disabled by default (None). If both min_epochs and\n",
      "                        min_steps are not specified, defaults to\n",
      "                        ``min_epochs`` = 1.\n",
      "  --max_steps MAX_STEPS\n",
      "                        Stop training after this number of steps. Disabled by\n",
      "                        default (None).\n",
      "  --min_steps MIN_STEPS\n",
      "                        Force training for at least these number of steps.\n",
      "                        Disabled by default (None).\n",
      "  --max_time MAX_TIME   Stop training after this amount of time has passed.\n",
      "                        Disabled by default (None). The time duration can be\n",
      "                        specified in the format DD:HH:MM:SS (days, hours,\n",
      "                        minutes seconds), as a :class:`datetime.timedelta`, or\n",
      "                        a dictionary with keys that will be passed to\n",
      "                        :class:`datetime.timedelta`.\n",
      "  --limit_train_batches LIMIT_TRAIN_BATCHES\n",
      "                        How much of training dataset to check (float =\n",
      "                        fraction, int = num_batches)\n",
      "  --limit_val_batches LIMIT_VAL_BATCHES\n",
      "                        How much of validation dataset to check (float =\n",
      "                        fraction, int = num_batches)\n",
      "  --limit_test_batches LIMIT_TEST_BATCHES\n",
      "                        How much of test dataset to check (float = fraction,\n",
      "                        int = num_batches)\n",
      "  --limit_predict_batches LIMIT_PREDICT_BATCHES\n",
      "                        How much of prediction dataset to check (float =\n",
      "                        fraction, int = num_batches)\n",
      "  --val_check_interval VAL_CHECK_INTERVAL\n",
      "                        How often to check the validation set. Use float to\n",
      "                        check within a training epoch, use int to check every\n",
      "                        n steps (batches).\n",
      "  --flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS\n",
      "                        How often to flush logs to disk (defaults to every 100\n",
      "                        steps).\n",
      "  --log_every_n_steps LOG_EVERY_N_STEPS\n",
      "                        How often to log within steps (defaults to every 50\n",
      "                        steps).\n",
      "  --accelerator ACCELERATOR\n",
      "                        Previously known as distributed_backend (dp, ddp,\n",
      "                        ddp2, etc...). Can also take in an accelerator object\n",
      "                        for custom hardware.\n",
      "  --sync_batchnorm [SYNC_BATCHNORM]\n",
      "                        Synchronize batch norm layers between process\n",
      "                        groups/whole world.\n",
      "  --precision PRECISION\n",
      "                        Double precision (64), full precision (32) or half\n",
      "                        precision (16). Can be used on CPU, GPU or TPUs.\n",
      "  --weights_summary WEIGHTS_SUMMARY\n",
      "                        Prints a summary of the weights when training begins.\n",
      "  --weights_save_path WEIGHTS_SAVE_PATH\n",
      "                        Where to save weights if specified. Will override\n",
      "                        default_root_dir for checkpoints only. Use this if for\n",
      "                        whatever reason you need the checkpoints stored in a\n",
      "                        different place than the logs written in\n",
      "                        `default_root_dir`. Can be remote file paths such as\n",
      "                        `s3://mybucket/path` or 'hdfs://path/' Defaults to\n",
      "                        `default_root_dir`.\n",
      "  --num_sanity_val_steps NUM_SANITY_VAL_STEPS\n",
      "                        Sanity check runs n validation batches before starting\n",
      "                        the training routine. Set it to `-1` to run all\n",
      "                        batches in all validation dataloaders.\n",
      "  --truncated_bptt_steps TRUNCATED_BPTT_STEPS\n",
      "                        Deprecated in v1.3 to be removed in 1.5. Please use :p\n",
      "                        aramref:`~pytorch_lightning.core.lightning.LightningMo\n",
      "                        dule.truncated_bptt_steps` instead.\n",
      "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
      "                        Path/URL of the checkpoint from which training is\n",
      "                        resumed. If there is no checkpoint file at the path,\n",
      "                        start from scratch. If resuming from mid-epoch\n",
      "                        checkpoint, training will start from the beginning of\n",
      "                        the next epoch.\n",
      "  --profiler PROFILER   To profile individual steps during training and assist\n",
      "                        in identifying bottlenecks.\n",
      "  --benchmark [BENCHMARK]\n",
      "                        If true enables cudnn.benchmark.\n",
      "  --deterministic [DETERMINISTIC]\n",
      "                        If true enables cudnn.deterministic.\n",
      "  --reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS\n",
      "                        Set to a non-negative integer to reload dataloaders\n",
      "                        every n epochs. Default: 0\n",
      "  --reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]\n",
      "                        Set to True to reload dataloaders every epoch. ..\n",
      "                        deprecated:: v1.4 ``reload_dataloaders_every_epoch``\n",
      "                        has been deprecated in v1.4 and will be removed in\n",
      "                        v1.6. Please use\n",
      "                        ``reload_dataloaders_every_n_epochs``.\n",
      "  --auto_lr_find [AUTO_LR_FIND]\n",
      "                        If set to True, will make trainer.tune() run a\n",
      "                        learning rate finder, trying to optimize initial\n",
      "                        learning for faster convergence. trainer.tune() method\n",
      "                        will set the suggested learning rate in self.lr or\n",
      "                        self.learning_rate in the LightningModule. To use a\n",
      "                        different key set a string instead of True with the\n",
      "                        key name.\n",
      "  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]\n",
      "                        Explicitly enables or disables sampler replacement. If\n",
      "                        not specified this will toggled automatically when DDP\n",
      "                        is used. By default it will add ``shuffle=True`` for\n",
      "                        train sampler and ``shuffle=False`` for val/test\n",
      "                        sampler. If you want to customize it, you can set\n",
      "                        ``replace_sampler_ddp=False`` and add your own\n",
      "                        distributed sampler.\n",
      "  --terminate_on_nan [TERMINATE_ON_NAN]\n",
      "                        If set to True, will terminate training (by raising a\n",
      "                        `ValueError`) at the end of each training batch, if\n",
      "                        any of the parameters or the loss are NaN or +/-inf.\n",
      "  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]\n",
      "                        If set to True, will `initially` run a batch size\n",
      "                        finder trying to find the largest batch size that fits\n",
      "                        into memory. The result will be stored in\n",
      "                        self.batch_size in the LightningModule. Additionally,\n",
      "                        can be set to either `power` that estimates the batch\n",
      "                        size through a power search or `binsearch` that\n",
      "                        estimates the batch size through a binary search.\n",
      "  --prepare_data_per_node [PREPARE_DATA_PER_NODE]\n",
      "                        If True, each LOCAL_RANK=0 will call prepare data.\n",
      "                        Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare\n",
      "                        data\n",
      "  --plugins PLUGINS     Plugins allow modification of core behavior like ddp\n",
      "                        and amp, and enable custom lightning plugins.\n",
      "  --amp_backend AMP_BACKEND\n",
      "                        The mixed precision backend to use (\"native\" or\n",
      "                        \"apex\")\n",
      "  --amp_level AMP_LEVEL\n",
      "                        The optimization level to use (O1, O2, etc...).\n",
      "  --distributed_backend DISTRIBUTED_BACKEND\n",
      "                        deprecated. Please use 'accelerator'\n",
      "  --move_metrics_to_cpu [MOVE_METRICS_TO_CPU]\n",
      "                        Whether to force internal logged metrics to be moved\n",
      "                        to cpu. This can save some gpu memory, but can make\n",
      "                        training slower. Use with attention.\n",
      "  --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE\n",
      "                        How to loop over the datasets when there are multiple\n",
      "                        train loaders. In 'max_size_cycle' mode, the trainer\n",
      "                        ends one epoch when the largest dataset is traversed,\n",
      "                        and smaller datasets reload when running out of their\n",
      "                        data. In 'min_size' mode, all the datasets reload when\n",
      "                        reaching the minimum length of datasets.\n",
      "  --stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]\n",
      "                        Whether to use `Stochastic Weight Averaging (SWA)\n",
      "                        <https://pytorch.org/blog/pytorch-1.6-now-includes-\n",
      "                        stochastic-weight-averaging/>_`\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "!python scripts/trainer.py \\\n",
    "    --gpus 1 \\\n",
    "    --size {model_size} \\\n",
    "    --dataset_path {dataset_location} \\\n",
    "    --resume_from '' \\\n",
    "    --batch 8 \\\n",
    "    --max_epochs 50 \\\n",
    "    --augment True \\\n",
    "    --start_kimg_count 1001"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using Alias-Free GAN version: 1.0.0\n",
      "Dataset path: /content/drive/MyDrive/alias-free-devel2/alias-free-gan/alias-free-gan-ci-files/flowers-test-dataset-32-256\n",
      "Initialized MultiResolutionDataset dataset with 32 images\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 17.3 M\n",
      "1 | g_ema         | Generator     | 17.3 M\n",
      "2 | discriminator | Discriminator | 28.9 M\n",
      "------------------------------------------------\n",
      "63.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "63.5 M    Total params\n",
      "253.864   Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:323: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "Training: -1it [00:00, ?it/s]\n",
      "\n",
      "AlignFreeGAN device: cuda:0\n",
      "\n",
      "\n",
      "Epoch 0:   0% 0/4 [00:00<00:00, 6335.81it/s]  /content/drive/MyDrive/alias-free-devel2/alias-free-gan/scripts/../src/stylegan2/op/conv2d_gradfix.py:89: UserWarning: conv2d_gradfix not supported on PyTorch 1.9.0+cu102. Falling back to torch.nn.functional.conv2d().\n",
      "  f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\n",
      "Epoch 49: 100% 4/4 [00:06<00:00,  1.34s/it, kimgs=1002.592, r_t_stat=0.781, ada_aug_p=0.002688]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}