{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/afg-lightning-devel-checkpoint/alias-free-gan-pytorch-lightning\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/afg-lightning-devel-checkpoint/alias-free-gan-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
      "\u001b[K     |████████████████████████████████| 813 kB 13.9 MB/s \n",
      "\u001b[?25hCollecting pytorch-lightning-bolts\n",
      "  Downloading pytorch_lightning_bolts-0.3.2-py3-none-any.whl (253 kB)\n",
      "\u001b[K     |████████████████████████████████| 253 kB 83.3 MB/s \n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.11.0-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 69.5 MB/s \n",
      "\u001b[?25hCollecting ninja\n",
      "  Downloading ninja-1.10.0.post3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 57.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (0.99)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.23)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 61.5 MB/s \n",
      "\u001b[?25hCollecting pyhocon\n",
      "  Downloading pyhocon-0.3.58.tar.gz (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 75.1 MB/s \n",
      "\u001b[?25hCollecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.1 MB 82 kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.2.0)\n",
      "Collecting tensorboard!=2.5.0,>=2.2.0\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 51.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
      "Collecting torchmetrics>=0.2.0\n",
      "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n",
      "\u001b[K     |████████████████████████████████| 234 kB 67.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu102)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 72.2 MB/s \n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 41.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.41.1)\n",
      "Collecting pyDeprecate==0.3.0\n",
      "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting PyYAML<=5.4.1,>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 55.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (7.1.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 55.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.17.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.34.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.32.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n",
      "Collecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 8.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.10.33-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 52.7 MB/s \n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting configparser>=3.8.1\n",
      "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
      "\u001b[K     |████████████████████████████████| 170 kB 78.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "  Downloading sentry_sdk-1.3.0-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 68.0 MB/s \n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.5 MB/s \n",
      "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
      "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (21.2.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.8.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.10.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 66.6 MB/s \n",
      "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 74.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.5.0)\n",
      "Building wheels for collected packages: future, subprocess32, pyhocon, pathtools\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=2d073c1aa0bea76806956a1d0b3d5b908ea9dadbcfef7f79059d33c7aecb7e59\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=9913e4f8005ab75972fed93bacd1616b59ce5f107d168f4a91b58367811d930d\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
      "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyhocon: filename=pyhocon-0.3.58-py3-none-any.whl size=19889 sha256=3c9a165ab7c5e3fd5141710d3649553eec10bb68f3f9e93b001fd71adedec792\n",
      "  Stored in directory: /root/.cache/pip/wheels/cb/20/f9/ff360765ce6f9fc078d6599c10a8f36496e5b5011a29df1ae3\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=9f3be7ad9181cc86357637f406ebbf52653555ec4fdfe4d86bbc684020b4f18c\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built future subprocess32 pyhocon pathtools\n",
      "Installing collected packages: multidict, yarl, async-timeout, smmap, fsspec, aiohttp, torchmetrics, tensorboard, PyYAML, pyDeprecate, gitdb, future, subprocess32, shortuuid, sentry-sdk, pytorch-lightning, pathtools, GitPython, docker-pycreds, configparser, wandb, pytorch-lightning-bolts, pyhocon, pydantic, opencv-python-headless, ninja\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.5.0\n",
      "    Uninstalling tensorboard-2.5.0:\n",
      "      Successfully uninstalled tensorboard-2.5.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.5.0 requires tensorboard~=2.5, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
      "Successfully installed GitPython-3.1.18 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 configparser-5.0.2 docker-pycreds-0.4.0 fsspec-2021.7.0 future-0.18.2 gitdb-4.0.7 multidict-5.1.0 ninja-1.10.0.post3 opencv-python-headless-4.5.3.56 pathtools-0.1.2 pyDeprecate-0.3.0 pydantic-1.8.2 pyhocon-0.3.58 pytorch-lightning-1.3.8 pytorch-lightning-bolts-0.3.2 sentry-sdk-1.3.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 tensorboard-2.4.1 torchmetrics-0.4.1 wandb-0.10.33 yarl-1.6.3\n",
      "Collecting torch-xla==1.9\n",
      "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 149.9 MB 70 kB/s \n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch==1.9 (from versions: 0.1.2, 1.0.2)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pytorch==1.9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python install.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: trainer.py [-h] --dataset_path DATASET_PATH --size SIZE [--batch BATCH]\n",
      "                  [--n_samples N_SAMPLES] [--lr_g LR_G] [--lr_d LR_D]\n",
      "                  [--d_reg_every D_REG_EVERY] [--r1 R1] [--augment AUGMENT]\n",
      "                  [--argument_p ARGUMENT_P] [--ada_target ADA_TARGET]\n",
      "                  [--ada_length ADA_LENGTH] [--ada_every ADA_EVERY]\n",
      "                  [--logger [LOGGER]]\n",
      "                  [--checkpoint_callback [CHECKPOINT_CALLBACK]]\n",
      "                  [--default_root_dir DEFAULT_ROOT_DIR]\n",
      "                  [--gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                  [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                  [--process_position PROCESS_POSITION]\n",
      "                  [--num_nodes NUM_NODES] [--num_processes NUM_PROCESSES]\n",
      "                  [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]]\n",
      "                  [--tpu_cores TPU_CORES] [--log_gpu_memory LOG_GPU_MEMORY]\n",
      "                  [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]\n",
      "                  [--overfit_batches OVERFIT_BATCHES]\n",
      "                  [--track_grad_norm TRACK_GRAD_NORM]\n",
      "                  [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                  [--fast_dev_run [FAST_DEV_RUN]]\n",
      "                  [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                  [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                  [--max_steps MAX_STEPS] [--min_steps MIN_STEPS]\n",
      "                  [--max_time MAX_TIME]\n",
      "                  [--limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                  [--limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                  [--limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                  [--limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                  [--val_check_interval VAL_CHECK_INTERVAL]\n",
      "                  [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]\n",
      "                  [--log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                  [--accelerator ACCELERATOR]\n",
      "                  [--sync_batchnorm [SYNC_BATCHNORM]] [--precision PRECISION]\n",
      "                  [--weights_summary WEIGHTS_SUMMARY]\n",
      "                  [--weights_save_path WEIGHTS_SAVE_PATH]\n",
      "                  [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                  [--truncated_bptt_steps TRUNCATED_BPTT_STEPS]\n",
      "                  [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                  [--profiler PROFILER] [--benchmark [BENCHMARK]]\n",
      "                  [--deterministic [DETERMINISTIC]]\n",
      "                  [--reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]]\n",
      "                  [--auto_lr_find [AUTO_LR_FIND]]\n",
      "                  [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]\n",
      "                  [--terminate_on_nan [TERMINATE_ON_NAN]]\n",
      "                  [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
      "                  [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]\n",
      "                  [--plugins PLUGINS] [--amp_backend AMP_BACKEND]\n",
      "                  [--amp_level AMP_LEVEL]\n",
      "                  [--distributed_backend DISTRIBUTED_BACKEND]\n",
      "                  [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
      "                  [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]\n",
      "                  [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "Trainer Script:\n",
      "  --dataset_path DATASET_PATH\n",
      "                        Path to dataset. Required!\n",
      "\n",
      "AliasFreeGAN Model:\n",
      "  --size SIZE           Pixel dimension of model. Must be 256, 512, or 1024.\n",
      "                        Required!\n",
      "  --batch BATCH         Batch size. Will be overridden if\n",
      "                        --auto_scale_batch_size is used. (default: 16)\n",
      "  --n_samples N_SAMPLES\n",
      "                        Number of samples to generate in training process.\n",
      "                        (default: 9)\n",
      "  --lr_g LR_G           Generator learning rate. (default: 0.002)\n",
      "  --lr_d LR_D           Discriminator learning rate. (default: 0.002)\n",
      "  --d_reg_every D_REG_EVERY\n",
      "                        Regularize discriminator ever _ iters. (default: 16)\n",
      "  --r1 R1               R1 regularization weights. (default: 10.0)\n",
      "  --augment AUGMENT     Use augmentations. (default: False)\n",
      "  --argument_p ARGUMENT_P\n",
      "                        (default: 0.0)\n",
      "  --ada_target ADA_TARGET\n",
      "                        (default: 0.6)\n",
      "  --ada_length ADA_LENGTH\n",
      "                        (default: 500000)\n",
      "  --ada_every ADA_EVERY\n",
      "                        (default: 256)\n",
      "\n",
      "pl.Trainer:\n",
      "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\n",
      "                        experiment tracking. A ``True`` value uses the default\n",
      "                        ``TensorBoardLogger``. ``False`` will disable logging.\n",
      "  --checkpoint_callback [CHECKPOINT_CALLBACK]\n",
      "                        If ``True``, enable checkpointing. It will configure a\n",
      "                        default ModelCheckpoint callback if there is no user-\n",
      "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\n",
      "                        ng.trainer.trainer.Trainer.callbacks`.\n",
      "  --default_root_dir DEFAULT_ROOT_DIR\n",
      "                        Default path for logs and weights when no\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\n",
      "                        or 'hdfs://path/'\n",
      "  --gradient_clip_val GRADIENT_CLIP_VAL\n",
      "                        0 means don't clip.\n",
      "  --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n",
      "                        'value' means clip_by_value, 'norm' means\n",
      "                        clip_by_norm. Default: 'norm'\n",
      "  --process_position PROCESS_POSITION\n",
      "                        orders the progress bar when running multiple models\n",
      "                        on same machine.\n",
      "  --num_nodes NUM_NODES\n",
      "                        number of GPU nodes for distributed training.\n",
      "  --num_processes NUM_PROCESSES\n",
      "                        number of processes for distributed training with\n",
      "                        distributed_backend=\"ddp_cpu\"\n",
      "  --gpus GPUS           number of gpus to train on (int) or which GPUs to\n",
      "                        train on (list or str) applied per node\n",
      "  --auto_select_gpus [AUTO_SELECT_GPUS]\n",
      "                        If enabled and `gpus` is an integer, pick available\n",
      "                        gpus automatically. This is especially useful when\n",
      "                        GPUs are configured to be in \"exclusive mode\", such\n",
      "                        that only one process at a time can access them.\n",
      "  --tpu_cores TPU_CORES\n",
      "                        How many TPU cores to train on (1 or 8) / Single TPU\n",
      "                        to train on [1]\n",
      "  --log_gpu_memory LOG_GPU_MEMORY\n",
      "                        None, 'min_max', 'all'. Might slow performance\n",
      "  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE\n",
      "                        How often to refresh progress bar (in steps). Value\n",
      "                        ``0`` disables progress bar. Ignored when a custom\n",
      "                        progress bar is passed to\n",
      "                        :paramref:`~Trainer.callbacks`. Default: None, means a\n",
      "                        suitable value will be chosen based on the environment\n",
      "                        (terminal, Google COLAB, etc.).\n",
      "  --overfit_batches OVERFIT_BATCHES\n",
      "                        Overfit a fraction of training data (float) or a set\n",
      "                        number of batches (int).\n",
      "  --track_grad_norm TRACK_GRAD_NORM\n",
      "                        -1 no tracking. Otherwise tracks that p-norm. May be\n",
      "                        set to 'inf' infinity-norm.\n",
      "  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n",
      "                        Check val every n train epochs.\n",
      "  --fast_dev_run [FAST_DEV_RUN]\n",
      "                        runs n if set to ``n`` (int) else 1 if set to ``True``\n",
      "                        batch(es) of train, val and test to find any bugs (ie:\n",
      "                        a sort of unit test).\n",
      "  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n",
      "                        Accumulates grads every k batches or as set up in the\n",
      "                        dict.\n",
      "  --max_epochs MAX_EPOCHS\n",
      "                        Stop training once this number of epochs is reached.\n",
      "                        Disabled by default (None). If both max_epochs and\n",
      "                        max_steps are not specified, defaults to\n",
      "                        ``max_epochs`` = 1000.\n",
      "  --min_epochs MIN_EPOCHS\n",
      "                        Force training for at least these many epochs.\n",
      "                        Disabled by default (None). If both min_epochs and\n",
      "                        min_steps are not specified, defaults to\n",
      "                        ``min_epochs`` = 1.\n",
      "  --max_steps MAX_STEPS\n",
      "                        Stop training after this number of steps. Disabled by\n",
      "                        default (None).\n",
      "  --min_steps MIN_STEPS\n",
      "                        Force training for at least these number of steps.\n",
      "                        Disabled by default (None).\n",
      "  --max_time MAX_TIME   Stop training after this amount of time has passed.\n",
      "                        Disabled by default (None). The time duration can be\n",
      "                        specified in the format DD:HH:MM:SS (days, hours,\n",
      "                        minutes seconds), as a :class:`datetime.timedelta`, or\n",
      "                        a dictionary with keys that will be passed to\n",
      "                        :class:`datetime.timedelta`.\n",
      "  --limit_train_batches LIMIT_TRAIN_BATCHES\n",
      "                        How much of training dataset to check (float =\n",
      "                        fraction, int = num_batches)\n",
      "  --limit_val_batches LIMIT_VAL_BATCHES\n",
      "                        How much of validation dataset to check (float =\n",
      "                        fraction, int = num_batches)\n",
      "  --limit_test_batches LIMIT_TEST_BATCHES\n",
      "                        How much of test dataset to check (float = fraction,\n",
      "                        int = num_batches)\n",
      "  --limit_predict_batches LIMIT_PREDICT_BATCHES\n",
      "                        How much of prediction dataset to check (float =\n",
      "                        fraction, int = num_batches)\n",
      "  --val_check_interval VAL_CHECK_INTERVAL\n",
      "                        How often to check the validation set. Use float to\n",
      "                        check within a training epoch, use int to check every\n",
      "                        n steps (batches).\n",
      "  --flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS\n",
      "                        How often to flush logs to disk (defaults to every 100\n",
      "                        steps).\n",
      "  --log_every_n_steps LOG_EVERY_N_STEPS\n",
      "                        How often to log within steps (defaults to every 50\n",
      "                        steps).\n",
      "  --accelerator ACCELERATOR\n",
      "                        Previously known as distributed_backend (dp, ddp,\n",
      "                        ddp2, etc...). Can also take in an accelerator object\n",
      "                        for custom hardware.\n",
      "  --sync_batchnorm [SYNC_BATCHNORM]\n",
      "                        Synchronize batch norm layers between process\n",
      "                        groups/whole world.\n",
      "  --precision PRECISION\n",
      "                        Double precision (64), full precision (32) or half\n",
      "                        precision (16). Can be used on CPU, GPU or TPUs.\n",
      "  --weights_summary WEIGHTS_SUMMARY\n",
      "                        Prints a summary of the weights when training begins.\n",
      "  --weights_save_path WEIGHTS_SAVE_PATH\n",
      "                        Where to save weights if specified. Will override\n",
      "                        default_root_dir for checkpoints only. Use this if for\n",
      "                        whatever reason you need the checkpoints stored in a\n",
      "                        different place than the logs written in\n",
      "                        `default_root_dir`. Can be remote file paths such as\n",
      "                        `s3://mybucket/path` or 'hdfs://path/' Defaults to\n",
      "                        `default_root_dir`.\n",
      "  --num_sanity_val_steps NUM_SANITY_VAL_STEPS\n",
      "                        Sanity check runs n validation batches before starting\n",
      "                        the training routine. Set it to `-1` to run all\n",
      "                        batches in all validation dataloaders.\n",
      "  --truncated_bptt_steps TRUNCATED_BPTT_STEPS\n",
      "                        Deprecated in v1.3 to be removed in 1.5. Please use :p\n",
      "                        aramref:`~pytorch_lightning.core.lightning.LightningMo\n",
      "                        dule.truncated_bptt_steps` instead.\n",
      "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
      "                        Path/URL of the checkpoint from which training is\n",
      "                        resumed. If there is no checkpoint file at the path,\n",
      "                        start from scratch. If resuming from mid-epoch\n",
      "                        checkpoint, training will start from the beginning of\n",
      "                        the next epoch.\n",
      "  --profiler PROFILER   To profile individual steps during training and assist\n",
      "                        in identifying bottlenecks.\n",
      "  --benchmark [BENCHMARK]\n",
      "                        If true enables cudnn.benchmark.\n",
      "  --deterministic [DETERMINISTIC]\n",
      "                        If true enables cudnn.deterministic.\n",
      "  --reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]\n",
      "                        Set to True to reload dataloaders every epoch.\n",
      "  --auto_lr_find [AUTO_LR_FIND]\n",
      "                        If set to True, will make trainer.tune() run a\n",
      "                        learning rate finder, trying to optimize initial\n",
      "                        learning for faster convergence. trainer.tune() method\n",
      "                        will set the suggested learning rate in self.lr or\n",
      "                        self.learning_rate in the LightningModule. To use a\n",
      "                        different key set a string instead of True with the\n",
      "                        key name.\n",
      "  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]\n",
      "                        Explicitly enables or disables sampler replacement. If\n",
      "                        not specified this will toggled automatically when DDP\n",
      "                        is used. By default it will add ``shuffle=True`` for\n",
      "                        train sampler and ``shuffle=False`` for val/test\n",
      "                        sampler. If you want to customize it, you can set\n",
      "                        ``replace_sampler_ddp=False`` and add your own\n",
      "                        distributed sampler.\n",
      "  --terminate_on_nan [TERMINATE_ON_NAN]\n",
      "                        If set to True, will terminate training (by raising a\n",
      "                        `ValueError`) at the end of each training batch, if\n",
      "                        any of the parameters or the loss are NaN or +/-inf.\n",
      "  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]\n",
      "                        If set to True, will `initially` run a batch size\n",
      "                        finder trying to find the largest batch size that fits\n",
      "                        into memory. The result will be stored in\n",
      "                        self.batch_size in the LightningModule. Additionally,\n",
      "                        can be set to either `power` that estimates the batch\n",
      "                        size through a power search or `binsearch` that\n",
      "                        estimates the batch size through a binary search.\n",
      "  --prepare_data_per_node [PREPARE_DATA_PER_NODE]\n",
      "                        If True, each LOCAL_RANK=0 will call prepare data.\n",
      "                        Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare\n",
      "                        data\n",
      "  --plugins PLUGINS     Plugins allow modification of core behavior like ddp\n",
      "                        and amp, and enable custom lightning plugins.\n",
      "  --amp_backend AMP_BACKEND\n",
      "                        The mixed precision backend to use (\"native\" or\n",
      "                        \"apex\")\n",
      "  --amp_level AMP_LEVEL\n",
      "                        The optimization level to use (O1, O2, etc...).\n",
      "  --distributed_backend DISTRIBUTED_BACKEND\n",
      "                        deprecated. Please use 'accelerator'\n",
      "  --move_metrics_to_cpu [MOVE_METRICS_TO_CPU]\n",
      "                        Whether to force internal logged metrics to be moved\n",
      "                        to cpu. This can save some gpu memory, but can make\n",
      "                        training slower. Use with attention.\n",
      "  --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE\n",
      "                        How to loop over the datasets when there are multiple\n",
      "                        train loaders. In 'max_size_cycle' mode, the trainer\n",
      "                        ends one epoch when the largest dataset is traversed,\n",
      "                        and smaller datasets reload when running out of their\n",
      "                        data. In 'min_size' mode, all the datasets reload when\n",
      "                        reaching the minimum length of datasets.\n",
      "  --stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]\n",
      "                        Whether to use `Stochastic Weight Averaging (SWA)\n",
      "                        <https://pytorch.org/blog/pytorch-1.6-now-includes-\n",
      "                        stochastic-weight-averaging/>_`\n"
     ]
    }
   ],
   "source": [
    "!python scripts/trainer.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.11, pytest-3.6.4, py-1.10.0, pluggy-0.7.1\n",
      "rootdir: /content/drive/MyDrive/afg-lightning-devel-checkpoint/alias-free-gan-pytorch-lightning, inifile:\n",
      "plugins: typeguard-2.7.1\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test/cross/src/alias_free_gan_test.py ..\u001b[36m                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "test/cross/src/alias_free_gan_test.py::test_save_checkpoint\n",
      "  /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:597: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "    \"GPU available but not used. Set the gpus flag in your trainer\"\n",
      "\n",
      "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
      "\u001b[33m\u001b[1m===================== 2 passed, 1 warnings in 6.67 seconds =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test/cross/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash\n"
     ]
    }
   ],
   "source": [
    "!which bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash ./scripts/tpu_setup.sh\n",
    "import os\n",
    "with open('./scripts/tpu_setup.sh') as f:\n",
    "    os.environ.update(\n",
    "        line.replace('export ', '', 1).strip().split('=', 1) for line in f\n",
    "        if 'export' in line\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "!echo $USE_CPU_OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.11, pytest-3.6.4, py-1.10.0, pluggy-0.7.1\n",
      "rootdir: /content/drive/MyDrive/afg-lightning-devel-tpu/alias-free-gan-pytorch-lightning, inifile:\n",
      "plugins: typeguard-2.7.1\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test/cross/src/alias_free_gan_test.py .\u001b[36m                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 5.44 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !bash ./scripts/tpu_setup.sh\n",
    "\n",
    "!python -m pytest test/cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.11, pytest-3.6.4, py-1.10.0, pluggy-0.7.1\n",
      "rootdir: /content/drive/MyDrive/afg-lightning-devel-tpu/alias-free-gan-pytorch-lightning, inifile:\n",
      "plugins: typeguard-2.7.1\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test/tpu/scripts/tpu_setup_test.py .\u001b[36m                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 0.25 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test/tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.11, pytest-3.6.4, py-1.10.0, pluggy-0.7.1\n",
      "rootdir: /content/drive/MyDrive/afg-lightning-devel-arrayfire/alias-free-gan-pytorch-lightning, inifile:\n",
      "plugins: typeguard-2.7.1\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "test/unit/alias_free_gan_test.py .\u001b[36m                                       [ 33%]\u001b[0m\n",
      "test/unit/op_test.py ..\u001b[36m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m========================== 3 passed in 39.42 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.11, pytest-3.6.4, py-1.10.0, pluggy-0.7.1\n",
      "rootdir: /content/drive/MyDrive/afg-lightning-devel-arrayfire/alias-free-gan-pytorch-lightning, inifile:\n",
      "plugins: typeguard-2.7.1\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test/unit/op_test.py ..\u001b[36m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m========================== 2 passed in 64.39 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test/unit/op_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}